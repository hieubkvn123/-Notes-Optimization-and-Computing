\newcommand{\cvxopt}{{\bf CVXOPT}}
\newcommand{\dom}{{\bf \mathrm{dom}}}
\section{Introduction}
\begin{definition}[Optimization Problem]
	Generally, an optimization problem is defined as follows:
	\begin{equation}\begin{aligned}
		\textup{minimize}: 		& \ f_0(x) \\
		\textup{subject to}: 	& \ f_i(x) \le 0, \quad i = 1, \dots, m \\
								& \ h_i(x) = 0, \quad i = 1, \dots, p.	
	\end{aligned}\end{equation} 
	\noindent Where we have:
	\begin{enumerate}
		\item $x\in\R^n$ is the optimization variable.
		\item $f_0:\R^n\to\R$ is the opjective (cost function).
		\item $f_i:\R^n\to\R$ are inequality constraints.
		\item $h_i:\R^n\to\R$ are equality constraints.
	\end{enumerate} 
\end{definition} 

\begin{definition}[Convex Optimization Problem]
	\noindent An optimization problem is a \textbf{convex optimization problem} if:
	\begin{enumerate}
		\item $f_0, f_1, \dots, f_m$ are convex.
		\item Equality constraints are affine.
	\end{enumerate} 
\end{definition} 

\noindent The reason why we need convex optimization problems are:
\begin{enumerate}
	\item Convex optimization problems can be solved optimally (no local minima).
	\item Time required to solve convex optimization problems is polynomial (in terms of number of variables and constraints).
\end{enumerate} 

\subsection{Convex Sets}
\begin{definition}[Lines]
	Let $x_1, x_2\in\R^n$. A line passing through $x_1, x_2$ is defined as:
	\begin{equation}
		L(x_1, x_2) = \bigCurl{
			x\in\R^n: x = \theta x_1 + (1-\theta)x_2, \theta\in\R
		}.
	\end{equation}

	\noindent When $\theta\in(0,1)$, we restrict the line to the points between $x_1$ and $x_2$ (exclusive). 
\end{definition} 

\begin{definition}[Affine Sets]
	An affine set contains its elements' \textbf{affine combinations}: If $x_1, \dots, x_k$ belongs to an affine set $A$, then it contains the affine combination
	\begin{equation}
		\sum_{i=1}^k\theta_i x_i\in A, \quad \theta_i\in\R, \sum_{i=1}^k\theta_i = 1.
	\end{equation} 

	\noindent For example,
	\begin{enumerate}
		\item An empty set is affine because there is no point.
		\item A singleton is affine because there is only one point.
		\item A line (extends indefinitely) is affine.
		\item Any vector space is affine.
		\item Linear subspaces of a vector space is affine.	
	\end{enumerate} 
\end{definition} 

\begin{definition}[Convex Sets]
	A convex set contains its elements' \textbf{convex combinations}: If $x_1, \dots, x_k$ belongs to an affine set $A$, then it contains the convex combination
	\begin{equation}
		\sum_{i=1}^k\theta_i x_i\in A, \quad \theta_i\in [0, 1], \sum_{i=1}^k\theta_i = 1.	
	\end{equation}

	\noindent For example,
	\begin{enumerate}
		\item Norm ball $\bigCurl{x:\|x\|\le r}$ for a given norm $\|\cdot\|$, radius $r$.	
		\item Hyperplane $\bigCurl{x:a^\top x = b}$ for given $a, b$.
		\item Halfspace $\bigCurl{x:a^\top x \le b}$ for given $a, b$.
		\item Affine space $\bigCurl{x:Ax = b}$ for given $A, b$.
	\end{enumerate}  
\end{definition} 

\begin{definition}[Convex Hull]
	Given a discrete set $C=\{x_1, \dots, x_k\}$. The convex hull of $C$, denoted $\mathrm{conv}(C)$, is the set of all convex combinations of points in $C$:
	\begin{equation}
		\mathrm{conv}(C) = \biggCurl{
			\sum_{i=1}^k \theta_ix_i: x_i \in C, \theta_i \ge 0, \sum_{i=1}^k\theta_i = 1 
		}.
	\end{equation} 	

	\noindent Convex hulls are always convex.
\end{definition} 


\subsection{Convex Functions}
\begin{definition}[Convex Function]
	A function $f:\R\to\R$ is convex if
	\begin{enumerate}[label=(\roman*)]
		\item $\mathrm{dom} f$ is convex.
		\item $f(\alpha x + (1-\alpha)y) \le \alpha f(x) + (1-\alpha)f(y), \forall x, y\in\mathrm{dom} f, \alpha\in [0,1]$.
	\end{enumerate} 

	\noindent A function $f$ is concave if $-f$ is convex.
\end{definition}


\begin{definition}[Strictly Convex Function]
	A function $f:\R\to\R$ is convex if
	\begin{enumerate}[label=(\roman*)]
		\item $\mathrm{dom} f$ is convex.
		\item $f(\alpha x + (1-\alpha)y) \color{red}<\color{black} \alpha f(x) + (1-\alpha)f(y), \forall x, y\in\mathrm{dom} f, \alpha\in [0,1]$.
	\end{enumerate} 

	\noindent A strictly convex function implies a unique global minimum.
\end{definition}

\noindent\newline\textbf{Test for Convexity of Function}: We have the following tests for the convexity of any (real-valued vector) functions.
\begin{enumerate}			
	\item First-order derivative test.
	\item Second-order derivative test.
	\item Restriction to a line.
	\item Epigraph.
\end{enumerate} 

\begin{proposition}{Restriction to A Line}{restriction_to_line}
	A function $f:\R^n\to\R$ is convex iff $g:\R\to\R$, defined as $g(t)=f(x+t\nu)$ is convex in $t$ for any $x\in\mathrm{dom}f$ and $\nu\in \R^n$.	Furthermore,
	\begin{equation}
		\mathrm{dom} g = \bigCurl{
			t\in\R: x+tv \in \mathrm{dom}f
		}.
	\end{equation} 
\end{proposition} 


\begin{proof*}
	We prove from both directions.
	\begin{enumerate}[label=(\roman*)]
		\item \textbf{$f$ convex $\implies$ $g$ convex}: There are two sub-sections to prove - $\mathrm{dom}g$ is convex and $g(\alpha t_1 + (1-\alpha)t_2)\le \alpha g(t_1) + (1-\alpha)g(t_2), \ \forall \alpha\in(0,1); t_1, t_2\in \mathrm{dom}g$.	
		\item \textbf{$g$ convex $\implies$ $f$ convex}:
	\end{enumerate} 	
\end{proof*} 

\begin{proposition}{Second Order Derivative Test}{second_order_der_test}
	Let $f:\R^n\to\R$ be a twice-differentiable function. Then, $f$ is convex iff $\nabla^2 f(x)\succeq	0$ (positive semi-definite Hessian).
\end{proposition} 

