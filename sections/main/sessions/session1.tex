\newcommand{\cvxopt}{{\bf CVXOPT}}
\newcommand{\dom}{{\mathrm{\bf dom}}}
\section{Introduction}
\begin{definition}[Optimization Problem]
	Generally, an optimization problem is defined as follows:
	\begin{equation}\begin{aligned}
		\textup{minimize}: 		& \ f_0(x) \\
		\textup{subject to}: 	& \ f_i(x) \le 0, \quad i = 1, \dots, m \\
								& \ h_i(x) = 0, \quad i = 1, \dots, p.	
	\end{aligned}\end{equation} 
	\noindent Where we have:
	\begin{enumerate}
		\item $x\in\R^n$ is the optimization variable.
		\item $f_0:\R^n\to\R$ is the opjective (cost function).
		\item $f_i:\R^n\to\R$ are inequality constraints.
		\item $h_i:\R^n\to\R$ are equality constraints.
	\end{enumerate} 
\end{definition} 

\begin{definition}[Convex Optimization Problem]
	\noindent An optimization problem is a \textbf{convex optimization problem} if:
	\begin{enumerate}
		\item $f_0, f_1, \dots, f_m$ are convex.
		\item Equality constraints are affine.
	\end{enumerate} 
\end{definition} 

\noindent The reason why we need convex optimization problems are:
\begin{enumerate}
	\item Convex optimization problems can be solved optimally (no local minima).
	\item Time required to solve convex optimization problems is polynomial (in terms of number of variables and constraints).
\end{enumerate} 

\subsection{Convex Sets}
\begin{definition}[Lines]
	Let $x_1, x_2\in\R^n$. A line passing through $x_1, x_2$ is defined as:
	\begin{equation}
		L(x_1, x_2) = \bigCurl{
			x\in\R^n: x = \theta x_1 + (1-\theta)x_2, \theta\in\R
		}.
	\end{equation}

	\noindent When $\theta\in(0,1)$, we restrict the line to the points between $x_1$ and $x_2$ (exclusive). 
\end{definition} 

\begin{definition}[Affine Sets]
	An affine set contains its elements' \textbf{affine combinations}: If $x_1, \dots, x_k$ belongs to an affine set $A$, then it contains the affine combination
	\begin{equation}
		\sum_{i=1}^k\theta_i x_i\in A, \quad \theta_i\in\R, \sum_{i=1}^k\theta_i = 1.
	\end{equation} 

	\noindent For example,
	\begin{enumerate}
		\item An empty set is affine because there is no point.
		\item A singleton is affine because there is only one point.
		\item A line (extends indefinitely) is affine.
		\item Any vector space is affine.
		\item Linear subspaces of a vector space is affine.	
	\end{enumerate} 
\end{definition} 

\begin{definition}[Convex Sets]
	A convex set contains its elements' \textbf{convex combinations}: If $x_1, \dots, x_k$ belongs to an affine set $A$, then it contains the convex combination
	\begin{equation}
		\sum_{i=1}^k\theta_i x_i\in A, \quad \theta_i\in [0, 1], \sum_{i=1}^k\theta_i = 1.	
	\end{equation}

	\noindent For example,
	\begin{enumerate}
		\item Norm ball $\bigCurl{x:\|x\|\le r}$ for a given norm $\|\cdot\|$, radius $r$.	
		\item Hyperplane $\bigCurl{x:a^\top x = b}$ for given $a, b$.
		\item Halfspace $\bigCurl{x:a^\top x \le b}$ for given $a, b$.
		\item Affine space $\bigCurl{x:Ax = b}$ for given $A, b$.
	\end{enumerate}  
\end{definition} 

\begin{definition}[Convex Hull]
	Given a discrete set $C=\{x_1, \dots, x_k\}$. The convex hull of $C$, denoted $\mathrm{conv}(C)$, is the set of all convex combinations of points in $C$:
	\begin{equation}
		\mathrm{conv}(C) = \biggCurl{
			\sum_{i=1}^k \theta_ix_i: x_i \in C, \theta_i \ge 0, \sum_{i=1}^k\theta_i = 1 
		}.
	\end{equation} 	

	\noindent Convex hulls are always convex.
\end{definition} 


\subsection{Convex Functions}
\begin{definition}[Convex Function]
	A function $f:\R\to\R$ is convex if
	\begin{enumerate}[label=(\roman*)]
		\item $\dom f$ is convex.
		\item $f(\alpha x + (1-\alpha)y) \le \alpha f(x) + (1-\alpha)f(y), \forall x, y\in\dom f, \alpha\in [0,1]$.
	\end{enumerate} 

	\noindent A function $f$ is concave if $-f$ is convex.
\end{definition}


\begin{definition}[Strictly Convex Function]
	A function $f:\R\to\R$ is convex if
	\begin{enumerate}[label=(\roman*)]
		\item $\dom f$ is convex.
		\item $f(\alpha x + (1-\alpha)y) \color{red}<\color{black} \alpha f(x) + (1-\alpha)f(y), \forall x, y\in\dom f, \alpha\in [0,1]$.
	\end{enumerate} 

	\noindent A strictly convex function implies a unique global minimum.
\end{definition}

\noindent\newline\textbf{Test for Convexity of Function}: We have the following tests for the convexity of any (real-valued vector) functions.
\begin{enumerate}			
	\item First-order derivative test.
	\item Second-order derivative test.
	\item Restriction to a line.
	\item Epigraph.
\end{enumerate} 

\begin{proposition}{Restriction to A Line}{restriction_to_line}
	A function $f:\R^n\to\R$ is convex iff $g_{x, \nu}:\R\to\R$, defined as $g_{x,\nu}(t)=f(x+t\nu)$ is convex in $t$ for any $x\in\dom f$ and $\nu\in \R^n$.	Furthermore,
	\begin{equation}
		\dom g_{x, \nu} = \bigCurl{
			t\in\R: x+t\nu \in \dom f
		} \subset \R.
	\end{equation} 
\end{proposition} 


\begin{proof*}[Proposition \ref{prop:restriction_to_line}]
	We prove from both directions.
	\begin{enumerate}[label=(\roman*)]
		\item \textbf{$f$ convex $\implies$ $g_{x, \nu}$ convex}: There are two sub-sections to prove - $\dom g_{x,\nu}$ is convex and $g_{x, \nu}(\alpha t_1 + (1-\alpha)t_2)\le \alpha g_{x, \nu}(t_1) + (1-\alpha)g_{x, \nu}(t_2), \ \forall \alpha\in(0,1); t_1, t_2\in \dom g_{x, \nu}$.	
		\begin{itemize}
			\item $\dom g_{x, \nu}$ is convex: Let $t_1, t_2\in\dom g_{x, \nu}$. Then, we have $x+t_1\nu, x+t_2\nu\in\dom f$. Then, by convexity of $\dom f$, for all $\alpha\in(0,1)$, we have $x+[\alpha t_1 + (1-\alpha)t_2]\nu\in\dom f$. Hence, we have $\alpha t_1 + (1-\alpha)t_2\in\dom g_{x, \nu}$, making $\dom g_{x, \nu}$ convex.

			\item For $\alpha\in(0,1)$ and $t_1, t_2\in \dom g_{x, \nu}$: 
			\begin{align*}
				g_{x,\nu}(\alpha t_1 + (1-\alpha)t_2) &= f\bigRound{
					\alpha(x+t_1\nu) + (1-\alpha)(x+t_2\nu)
				} \\
				&\le \alpha f(x+t_1\nu) + (1-\alpha)f(x+t_2\nu)\\
				&= \alpha g(t_1) + (1-\alpha)g(t_2).
			\end{align*} 

			\noindent Hence, $g$ is convex.
		\end{itemize} 
		\item \textbf{$g_{x,\nu}$ convex $\implies$ $f$ convex}: This direction is proven similarly without much deviation.
	\end{enumerate} 	
\end{proof*} 

\begin{proposition}{First Order Derivative Test}{first_order_der_test}
	Let $f:\R^n\to\R$ be a differentiable function. Then, $f$ is convex iff the following are satisfied for all $x, y\in\dom f$:
	\begin{equation}
		\label{eq:first_order_der_test}
		f(y) \ge \nabla f(x)^\top (y-x) + f(x).
	\end{equation} 
\end{proposition} 

\begin{proof*}[Proposition \ref{prop:first_order_der_test}]
	We prove both directions separately.
	\begin{enumerate}[label=(\roman*)]
		\item $f$ convex $\implies$ Eqn. \eqref{eq:first_order_der_test} holds: Let $\alpha\in(0,1)$. Then for every $x, y\in\dom f$, we have:
		\begin{align*}
			f(\alpha y + (1-\alpha)x) &= f(x+\alpha(y-x)) \\
			 	&\le \alpha f(y) + (1-\alpha)f(x).
		\end{align*} 

		\noindent Set $\Delta=y-x$, we have:
		\begin{align*}
			f(x+\alpha\Delta) - f(x) &\le \alpha [f(y) - f(x)] \\
			\implies
			\frac{f(x+\alpha\Delta) - f(x)}{\alpha\Delta} &\le \frac{f(y) - f(x)}{\Delta}.
		\end{align*}

		\noindent Taking the limit as $\alpha\to 0$ from both sides, we have $\nabla f(x)^\top\le \frac{f(y)-f(x)}{y-x}$. Hence, we proved Eqn. \eqref{eq:first_order_der_test} holds.

		\item Eqn. \eqref{eq:first_order_der_test} holds $\implies$ $f$ convex: For any $\alpha\in(0,1)$. For any $x, y\in\dom f$, let $z=\alpha x + (1-\alpha)y$. Then, since Eqn.~\eqref{eq:first_order_der_test} holds, we have:
		\begin{align*}
			f(x) &\ge f(z) + \nabla f(z)^\top (x-z), \\
			f(y) &\ge f(z) + \nabla f(z)^\top (y-z).
		\end{align*} 

		\noindent Then, we have:
		\begin{align*}
			\alpha f(x) + (1-\alpha) f(y) &\ge f(z) + \nabla f(z)^\top [\alpha x + (1-\alpha)y - z] \\
			&= f(z) \\
			&= f(\alpha x + (1-\alpha)y).
		\end{align*} 

		\noindent Hence, $f$ is convex.
	\end{enumerate} 	
\end{proof*} 

\begin{proposition}{Second Order Derivative Test}{second_order_der_test}
	Let $f:\R^n\to\R$ be a twice-differentiable function. Then, $f$ is convex iff $\nabla^2 f(x)\succeq	0$ (positive semi-definite Hessian).
\end{proposition} 

\begin{proof*}[Proposition \ref{prop:second_order_der_test}]
	
\end{proof*} 


